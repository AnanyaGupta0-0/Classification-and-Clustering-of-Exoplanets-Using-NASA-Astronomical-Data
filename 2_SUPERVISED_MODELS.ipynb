{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Supervised Learning Models\n",
    "### Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,f1_score\n",
    "df_raw = pd.read_csv(r\"C:\\Users\\madha\\Downloads\\12310219-PA\\nasa_exoplanets.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "#### =======================================\n",
    "#### PLANET DISCOVERY METHOD CLASSIFICATION\n",
    "#### =======================================\n",
    "#### Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_A = [\n",
    "    'pl_orbper',\n",
    "    'pl_orbsmax',\n",
    "    'pl_rade',\n",
    "    'pl_bmasse',\n",
    "    'st_teff',\n",
    "    'st_mass',\n",
    "    'st_rad',\n",
    "    'sy_dist'\n",
    "]\n",
    "features_A = [f for f in features_A if f in df.columns]\n",
    "df = df_raw[features_A + [\"discoverymethod\"]].copy()\n",
    "print(\"Initial shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "#### Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(thresh=len(features_A) - 2)\n",
    "\n",
    "for col in features_A:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "print(\"After cleaning shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "#### Target Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_discovery_method(method):\n",
    "    method = str(method).lower()\n",
    "\n",
    "    if method == 'transit':\n",
    "        return 'Transit'\n",
    "    elif method == 'radial velocity':\n",
    "        return 'Radial Velocity'\n",
    "    elif 'timing' in method:\n",
    "        return 'Timing'\n",
    "    elif method == 'microlensing':\n",
    "        return 'Microlensing'\n",
    "    elif method == 'imaging':\n",
    "        return 'Imaging'\n",
    "    elif method == 'astrometry':\n",
    "        return 'Astrometry'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df[\"discovery_simple\"] = df[\"discoverymethod\"].apply(simplify_discovery_method)\n",
    "\n",
    "print(df[\"discovery_simple\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### Reduce Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df[\"discovery_simple\"].value_counts()\n",
    "valid_classes = class_counts[class_counts >= 100].index\n",
    "df = df[df[\"discovery_simple\"].isin(valid_classes)]\n",
    "\n",
    "print(df[\"discovery_simple\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Encoding and Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features_A]\n",
    "y = df['discovery_simple']\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Train Multiple Models (Using Pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_A = {\n",
    "    \"Logistic Regression\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LogisticRegression(max_iter=1000,class_weight=\"balanced\"))]),\n",
    "    \n",
    "    \"K-Nearest Neighbors\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", KNeighborsClassifier(n_neighbors=7, weights='distance'))\n",
    "    ]),\n",
    "    \n",
    "    \"Decision Tree\": Pipeline([\n",
    "        (\"model\", DecisionTreeClassifier(max_depth=5,class_weight=\"balanced\", random_state=42))\n",
    "    ])\n",
    "}\n",
    "results_A = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Detailed Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models_A.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    results_A[name] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "results_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = models_A[\"Decision Tree\"]\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "sns.heatmap(cm, annot=True, fmt=\".2f\")\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "####  Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_A = models_A[\"Decision Tree\"].named_steps[\"model\"]\n",
    "# Feature importance\n",
    "importance_A = dt_A.feature_importances_\n",
    "fi_A = pd.DataFrame({\n",
    "    \"Feature\": features_A,\n",
    "    \"Importance\": importance_A\n",
    "}).sort_values(by=\"Importance\", ascending=True)\n",
    "print(fi_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "plt.hlines(\n",
    "    y=fi_A[\"Feature\"],\n",
    "    xmin=0,\n",
    "    xmax=fi_A[\"Importance\"]\n",
    ")\n",
    "plt.plot(\n",
    "    fi_A[\"Importance\"],\n",
    "    fi_A[\"Feature\"],\n",
    "    \"o\"\n",
    ")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"Feature Importance (Discovery Method)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "#### Domain Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(\n",
    "    x=\"discoverymethod\",\n",
    "    y=\"pl_rade\",\n",
    "    data=df\n",
    ")\n",
    "plt.title(\"Planet Radius vs Discovery Method\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### =========================================\n",
    "###        TEMPERATURE CLASSIFICATION\n",
    "### =========================================\n",
    "#### Feature Selection & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_B = [\n",
    "    \"pl_orbper\",\n",
    "    \"pl_rade\",\n",
    "    \"pl_bmasse\",\n",
    "    \"st_teff\",\n",
    "    \"st_mass\"\n",
    "]\n",
    "\n",
    "df_B = df_raw[features_B + [\"pl_eqt\"]].copy()\n",
    "\n",
    "# Fill missing values (simple & beginner-friendly)\n",
    "for col in features_B + [\"pl_eqt\"]:\n",
    "    df_B[col] = df_B[col].fillna(df_B[col].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "#### Create Temperature Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_class(temp):\n",
    "    if temp <= 300:\n",
    "        return \"Non-Hot\"   \n",
    "    else:\n",
    "        return \"Hot\"\n",
    "\n",
    "df_B[\"temp_class\"] = df_B[\"pl_eqt\"].apply(temperature_class)\n",
    "df_B.drop(columns=[\"pl_eqt\"], inplace=True)\n",
    "\n",
    "print(df_B[\"temp_class\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Encode Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_temp = LabelEncoder()\n",
    "df_B[\"temp_class\"] = le_temp.fit_transform(df_B[\"temp_class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "#### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_B[features_B]\n",
    "y = df_B[\"temp_class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "#### Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_B = {\n",
    "    \"Logistic Regression\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            class_weight=\"balanced\"\n",
    "        ))\n",
    "    ]),\n",
    "\n",
    "    \"Naive Bayes\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", GaussianNB())\n",
    "    ]),\n",
    "\n",
    "    \"Decision Tree\": Pipeline([\n",
    "        (\"model\", DecisionTreeClassifier(\n",
    "            max_depth=5,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "#### Train & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_B = {}\n",
    "\n",
    "for name, model in models_B.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    results_B[name] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "results_B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "#### Detailed Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_temp = models_B[\"Decision Tree\"]\n",
    "y_pred_dt = dt_temp.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_dt, normalize=\"true\")\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
    "plt.title(\"Temperature Classification – Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_B = models_B[\"Decision Tree\"].named_steps[\"model\"]\n",
    "\n",
    "importance_B = dt_B.feature_importances_\n",
    "\n",
    "fi_B = pd.DataFrame({\n",
    "    \"Feature\": features_B,\n",
    "    \"Importance\": importance_B\n",
    "}).sort_values(by=\"Importance\", ascending=True)\n",
    "\n",
    "print(fi_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_sorted = fi_B.sort_values(\"Importance\", ascending=False)\n",
    "fi_sorted[\"Cumulative\"] = fi_sorted[\"Importance\"].cumsum()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(\n",
    "    fi_sorted[\"Cumulative\"],\n",
    "    marker=\"o\"\n",
    ")\n",
    "\n",
    "plt.xticks(range(len(fi_sorted)), fi_sorted[\"Feature\"],  ha=\"right\")\n",
    "plt.ylabel(\"Cumulative Importance\")\n",
    "plt.title(\"Cumulative Feature Importance — Temperature Classification\")\n",
    "\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "#### Domain Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_plot = df_B.copy()\n",
    "df_temp_plot[\"temp_label\"] = le_temp.inverse_transform(df_temp_plot[\"temp_class\"])\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "sns.violinplot(\n",
    "    x=\"temp_label\",\n",
    "    y=\"st_teff\",\n",
    "    data=df_temp_plot,\n",
    "    inner=\"quartile\",\n",
    "    cut=0\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Planet Temperature Class\")\n",
    "plt.ylabel(\"Stellar Effective Temperature (K)\")\n",
    "plt.title(\"Stellar Temperature Distribution by Planet Temperature Class\", pad=8)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## MODEL COMPARISON\n",
    "#### Prediction Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression probabilities\n",
    "lr_B = models_B[\"Logistic Regression\"]\n",
    "lr_probs_B = lr_B.predict_proba(X_test)\n",
    "# Naive Bayes probabilities\n",
    "nb_B = models_B[\"Naive Bayes\"]\n",
    "nb_probs_B = nb_B.predict_proba(X_test)\n",
    "# Decision Tree probabilities\n",
    "dt_B = models_B[\"Decision Tree\"]\n",
    "dt_probs_B = dt_B.predict_proba(X_test)\n",
    "\n",
    "# Confidence scores \n",
    "lr_conf_B = lr_probs_B.max(axis=1)\n",
    "nb_conf_B = nb_probs_B.max(axis=1)\n",
    "dt_conf_B = dt_probs_B.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "sns.kdeplot(lr_conf_B, label=\"Logistic Regression\", fill=True)\n",
    "sns.kdeplot(nb_conf_B, label=\"Naive Bayes\", fill=True)\n",
    "sns.kdeplot(dt_conf_B, label=\"Decision Tree\", fill=True)\n",
    "\n",
    "plt.xlabel(\"Prediction Confidence\")\n",
    "plt.title(\"Model Confidence Comparison\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
